# Part3:APPLICATIONS OF WORLD MODELS IN AI
***
## 3.6 Interpretable and Trustworthy World Models
***
* General agents need world models [paper](https://icml.cc/virtual/2025/poster/44620) ![会议徽章](https://img.shields.io/badge/ICML-2025-blue)
* A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment [paper](https://arxiv.org/abs/2412.07446) ![会议徽章](https://img.shields.io/badge/arXiv-2024.12-red)
* Transformers Use Causal World Models in Maze-Solving Tasks [paper](https://iclr.cc/virtual/2025/37549) ![会议徽章](https://img.shields.io/badge/ICLR-2025-blue)
* When Do Neural Networks Learn World Models? [paper](https://arxiv.org/abs/2502.09297) ![会议徽章](https://img.shields.io/badge/arXiv-2025.02-red)
* Linear spatial world models emerge in large language models. [paper](https://arxiv.org/abs/2506.02996) ![会议徽章](https://img.shields.io/badge/arXiv-2025.06-red)
* Revisiting the othello world model hypothesis. [paper](https://arxiv.org/abs/2503.04421) ![会议徽章](https://img.shields.io/badge/arXiv-2025.03-red)
* Scaling laws for pre-training agents and world models. [paper](https://arxiv.org/abs/2411.04434) ![会议徽章](https://img.shields.io/badge/arXiv-2024.11-red)
* How hard is it to confuse a world model? [paper](https://arxiv.org/abs/2510.21232) ![会议徽章](https://img.shields.io/badge/arXiv-2025.10-red)
* Utilizing world models for adaptively covariate acquisition under limited budget for causal decision making. [paper](https://iclr.cc/virtual/2025/37518) ![会议徽章](https://img.shields.io/badge/ICLR-2025-blue)
***
# Part4:BENCHMARK OF WORLD MODELS
***
## 4.1 Benchmark Datasets & Evaluation Metrics
***
* Frozen in time: A joint video and image encoder for end-to-end retrieval. [paper](https://openaccess.thecvf.com/content/ICCV2021/html/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.html) ![会议徽章](https://img.shields.io/badge/ICCV-2021-blue)
* Panda-70m: Captioning 70m videos with multiple cross-modality teachers. [paper](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Panda-70M_Captioning_70M_Videos_with_Multiple_Cross-Modality_Teachers_CVPR_2024_paper.html) ![会议徽章](https://img.shields.io/badge/CVPR-2024-blue)
* Ego4d: Around the world in 3,000 hours of egocentric video. [paper](https://openaccess.thecvf.com/content/CVPR2022/html/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.html) ![会议徽章](https://img.shields.io/badge/CVPR-2022-blue)
* Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. [paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.html) ![会议徽章](https://img.shields.io/badge/ICCV-2019-blue)
* Worldscore:A unified evaluation benchmark for world generation. [paper](https://arxiv.org/abs/2504.00983) ![会议徽章](https://img.shields.io/badge/arXiv-2025.04-red)
* Open x-embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0. [paper](https://ieeexplore.ieee.org/document/10611477) ![会议徽章](https://img.shields.io/badge/ICRA-2024-blue)
* Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. [paper](https://aclanthology.org/2020.emnlp-main.356/) ![会议徽章](https://img.shields.io/badge/EMNLP-2020-blue)
* Ewmbench: Evaluating scene, motion, and semantic quality in embodied world models. [paper](https://arxiv.org/abs/2505.09694) ![会议徽章](https://img.shields.io/badge/arXiv-2025.05-red)
* Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. [paper](https://arxiv.org/abs/1910.10897) ![会议徽章](https://img.shields.io/badge/arXiv-2019.10-red)
* nuscenes: A multimodal dataset for autonomous driving. [paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.html) ![会议徽章](https://img.shields.io/badge/CVPR-2020-blue)
* ACT-Bench: Towards Action Controllable World Models for Autonomous Driving. [paper](https://arxiv.org/abs/2412.05337) ![会议徽章](https://img.shields.io/badge/arXiv-2024.12-red)
* Jump cell painting dataset: morphological impact of 136,000 chemical and genetic perturbations. [paper](https://www.biorxiv.org/content/10.1101/2023.03.23.534023v1) ![会议徽章](https://img.shields.io/badge/BioRxiv-2023-blue)
* A machine learning modelto predict hepatocellular carcinoma response to transcatheter arterial chemoembolization.  [paper](https://pubs.rsna.org/doi/full/10.1148/ryai.2019180021) ![会议徽章](https://img.shields.io/badge/Radiology-2019-purple)
* The arcade learning environment: An evaluation platform for general agents. [paper](https://arxiv.org/abs/1207.4708) ![会议徽章](https://img.shields.io/badge/arXiv-2012-red)
* Minerl: A large-scale dataset of minecraft demonstrations. [paper](https://www.ijcai.org/proceedings/2019/339) ![会议徽章](https://img.shields.io/badge/IJCAI-2019-blue)
* OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. [paper](https://neurips.cc/virtual/2024/poster/97468) ![会议徽章](https://img.shields.io/badge/NeurIPS-2024-blue)
* Windows agent arena: Evaluating multi-modal OS agents at scale.  [paper](https://icml.cc/virtual/2025/poster/45035) ![会议徽章](https://img.shields.io/badge/ICML-2025-blue)

